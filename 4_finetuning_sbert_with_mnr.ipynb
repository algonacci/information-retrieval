{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debe5699",
   "metadata": {},
   "source": [
    "# Finetuning SBERT for Semantic search using MNR loss\n",
    "\n",
    "In this notebook we will finetune the bert-base model for semantic search using Multiple Negative Ranking loss.\n",
    "\n",
    "This will be mostly similar to the finetuning we did in the previous notebook. The main changes are:\n",
    "- We won't use a fully connected layer on top of the embeddings now\n",
    "- We will use Multiple Negative Ranking loss\n",
    "- We will compute cosine similarity between the pooled *u* and *v* embedding and use that in MNR\n",
    "\n",
    "> This loss expects as input a batch consisting of sentence pairs (a_1, p_1), (a_2, p_2)..., (a_n, p_n)\n",
    "where we assume that (a_i, p_i) are a positive pair and (a_i, p_j) for i!=j a negative pair.\n",
    ">\n",
    "> For each a_i, it uses all other p_j as negative samples, i.e., for a_i, we have 1 positive example (p_i) and\n",
    "n-1 negative examples (p_j). It then minimizes the negative log-likehood for softmax normalized scores.\n",
    "This loss function works great to train embeddings for retrieval setups where you have positive pairs (e.g. (query, relevant_doc)) as it will sample in each batch n-1 negative docs randomly.\n",
    ">\n",
    "> The performance usually increases with increasing batch sizes.\n",
    ">\n",
    "> For more information, see: https://arxiv.org/pdf/1705.00652.pdf<br>\n",
    "(Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4)\n",
    ">\n",
    "> You can also provide one or multiple hard negatives per anchor-positive pair by structering the data like this:\n",
    "(a_1, p_1, n_1), (a_2, p_2, n_2)\n",
    ">\n",
    "> Here, n_1 is a hard negative for (a_1, p_1). The loss will use for the pair (a_i, p_i) all p_j (j!=i) and all n_j as negatives.\n",
    ">\n",
    "> Source: https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py\n",
    "\n",
    "We will just use the (anchor, postive) pairs in our training here. We will discuss hard negatives later in the series.\n",
    "\n",
    "Since we just need the (achor, positive) pairs, we will filter the snli dataset with label=0 for `entails`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf12a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/utsav/.cache/huggingface/modules/datasets_modules/datasets/snli/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b (last modified on Thu Jul 21 14:15:05 2022) since it couldn't be found locally at snli., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset snli (/home/utsav/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0e00c116a542ffbd4ce2296d020603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(183416,\n",
       " {'premise': 'A person on a horse jumps over a broken down airplane.',\n",
       "  'hypothesis': 'A person is outdoors, on a horse.',\n",
       "  'label': 0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset('snli', split='train')\n",
    "\n",
    "# there are some pairs of \"premise\" and \"hypothesis\" which haven't been\n",
    "# labeled in this dataset, we will filter those out first\n",
    "dataset = dataset.filter(lambda x: True if x[\"label\"] == 0 else False)\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac56673",
   "metadata": {},
   "source": [
    "Let's look at an example of calculating MLR for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff2b3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "anchors, positives = [], []\n",
    "for i in range(batch_size):\n",
    "    anchors.append(dataset[i][\"premise\"])\n",
    "    positives.append(dataset[i][\"hypothesis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114ee679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A person on a horse jumps over a broken down airplane.',\n",
       " 'Children smiling and waving at camera',\n",
       " 'A boy is jumping on skateboard in the middle of a red bridge.',\n",
       " 'Two blond women are hugging one another.',\n",
       " 'A few people in a restaurant setting, one of them is drinking orange juice.',\n",
       " 'An older man is drinking orange juice at a restaurant.',\n",
       " 'A man with blond-hair, and a brown shirt drinking out of a public water fountain.',\n",
       " 'Two women who just had lunch hugging and saying goodbye.',\n",
       " 'Two women, holding food carryout containers, hug.',\n",
       " 'A Little League team tries to catch a runner sliding into a base in an afternoon game.',\n",
       " 'The school is having a special event in order to show the american culture on how other cultures are dealt with in parties.',\n",
       " 'High fashion ladies wait outside a tram beside a crowd of people in the city.',\n",
       " 'A man, woman, and child enjoying themselves on a beach.',\n",
       " 'People waiting to get on a train or just getting off.',\n",
       " 'People waiting to get on a train or just getting off.',\n",
       " 'A couple playing with a little boy on the beach.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd85ade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A person is outdoors, on a horse.',\n",
       " 'There are children present',\n",
       " 'The boy does a skateboarding trick.',\n",
       " 'There are women showing affection.',\n",
       " 'The diners are at a restaurant.',\n",
       " 'A man is drinking juice.',\n",
       " 'A blond man drinking water from a fountain.',\n",
       " 'There are two woman in this picture.',\n",
       " 'Two women hug each other.',\n",
       " 'A team is trying to tag a runner out.',\n",
       " 'A school is hosting an event.',\n",
       " 'Women are waiting by a tram.',\n",
       " 'A family of three is at the beach.',\n",
       " 'There are people just getting on a train',\n",
       " 'There are people waiting on a train.',\n",
       " 'A couple are playing with a young child outside.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformers, util\n",
    "\n",
    "model = SentenceTransformers(\"\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "anchor_encodings = model.encode(anchors)\n",
    "postiive_encodings = model.encode(positives)\n",
    "\n",
    "similarity_matrix = util.cos_sim(anchor_embeddings, positive_embeddings)\n",
    "target = torch.tensor(range(len(anchor_embeddings)), dtype=torch.long)\n",
    "\n",
    "loss_fn(similarity_matrix, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
