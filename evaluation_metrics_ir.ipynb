{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3bb69e",
   "metadata": {},
   "source": [
    "# Evaluation for IR\n",
    "\n",
    "Here we are only going to look at offline evaluation metrics. We won't cover online evaluation techniques like click-through rate or running A/B tests where a subset of users are presented results from a newer test system.\n",
    "\n",
    "## Offline evaluation\n",
    "\n",
    "The idea behind these evaluations is to quantitatively compare multiple IR models. Typically we have a labelled dataset where we have queries mapped to relvevant documents. The documents could either be graded or non-graded(binary). For example, a graded relevance score could be on a scale of 0-5 with 5 being the most relevant.\n",
    "\n",
    "Labelled data typicalls comes form manual annotations or click data.\n",
    "\n",
    "It's also not necessary to have just 1 document tagged as relevant for each query. TREC collections have 100s of documents tagged as relevant per quuery. On the other hand MSMARCO has ~1 document tagged as relevant per query. This tends to be quite noisy but easy to scale hence more variability in the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90af75",
   "metadata": {},
   "source": [
    "## Binary labels\n",
    "\n",
    "### Precision@k\n",
    "\n",
    "Precision@k corresponds to the number of relevant documents among top k retrieved documents.\n",
    "\n",
    "$$ Precision@k = \\frac{TP@k}{TP@k + FP@k} $$\n",
    "\n",
    "![](assets/precision@k.png)\n",
    "\n",
    "Precision fails to take into account the ordering of the relevant documents. For example consider the models A and B (Fig 2) where model A outputs `[1,1,1,0,0](first 3 relevant)` and model B outputs `[0,0,1,1,1](indices 3-5 relevant)`; both the models get the same score `Precision@5=3/5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52b215",
   "metadata": {},
   "source": [
    "### MAP@k: Mean Average Precision\n",
    "\n",
    "*Average Precision(AP)* evaluates whether all the relevant items selected by the model are ranked higher or not.\n",
    "Every time we find a relevant document, we look at the full picture of what came before.\n",
    "\n",
    "MAP squeezes complex evaluation into a single number. It's essentially the mean of AP over all the queries.\n",
    "\n",
    "$$ AP@k(q) = \\frac{\\sum_{i=1}^k P(q)@i * rel(q)_i}{|rel(q)|} $$\n",
    "\n",
    "$$ MAP@k(Q) = \\frac{1}{|Q|} * \\sum_{q\\in Q} AP@k(q) $$\n",
    "\n",
    "|Q| - Number of queries<br>\n",
    "𝑃(𝑞)@𝑖 - Precision of query q after first i documents<br>\n",
    "𝑟𝑒𝑙(𝑞)𝑖 - Binary relevance of doc at position i<br>\n",
    "|𝑟𝑒𝑙(𝑞)| - Number of relevant documents\n",
    "\n",
    "![](assets/ap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4eec9",
   "metadata": {},
   "source": [
    "### MRR: Mean Reciprocal Rank\n",
    "\n",
    "MRR puts focus on the first relevant document. It's applicable when the system needs to return only the top matching document or the user only cares about the top result.\n",
    "\n",
    "$$ MRR(Q) = \\frac{1}{|Q|} * \\sum_{q\\in Q} \\frac{1}{FirstRank(q)} $$\n",
    "\n",
    "|Q| - Number of queries<br>\n",
    "*FirstRank(q)* - Returns the rank of first relevant dcuments for 1 query\n",
    "\n",
    "MRR(for 1 query) for different positions can be seen below. Notice the sharp fall in the values beyond the top few places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64212097",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "plt.figure(figsize=(16, 5));\n",
    "plt.plot(range(1, N+1), [1/n for n in range(1, N+1)], c=\"b\", label=\"1/x\");\n",
    "plt.legend();\n",
    "plt.xticks(range(1, N+1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6ca86",
   "metadata": {},
   "source": [
    "** Note that the metrics related to **Recall** are not used in general because it is easy to achieve a recall of 100% by returning all documents for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21d0d3",
   "metadata": {},
   "source": [
    "## Graded labels\n",
    "\n",
    "### nDCG@k: Normalized Discounted Cumulative Gain\n",
    "\n",
    "To understand nDCG, first let's look at CG i.e. Cumulative Gain. We simply add up the relevace scores for top k documents returned for a query.\n",
    "\n",
    "$$ CG@k(q) = \\sum_{i=1}^k rel(q)_i $$\n",
    "\n",
    "![](assets/cg@k.png)\n",
    "\n",
    "Note that CG does not take into account the position of the document. For example consider the models A and B where model A outputs `[5,2,4,0,1]` and model B outputs `[2,0,5,1,4]`; both the models get the same score `CG@5=12`.\n",
    "\n",
    "DCG i.e. Discounted Cumulative Gain improves on CG by adding a discounting factor for each position.\n",
    "\n",
    "$$ DCG@k(q) = \\sum_{i=1}^k \\frac{rel(q)_i}{log_2(i+1)} $$\n",
    "\n",
    "𝑟𝑒𝑙(𝑞)𝑖 - Graded relevance of doc at position i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _get_dcg_at_k(scores: List[int]) -> pd.DataFrame:\n",
    "    cols = [\"position(i)\", \"relevance(i)\", \"log2(i+1)\", \"relevance(i) / log2(i+1)\"]\n",
    "\n",
    "    data = []\n",
    "    for i, score in enumerate(scores):\n",
    "        position = i + 1\n",
    "        data.append((position, score, math.log2(position+1), score / math.log2(position+1)))\n",
    "\n",
    "    return pd.DataFrame(data, columns=cols)\n",
    "\n",
    "\n",
    "def get_dcg_at_k(scores: List[int]) -> None:\n",
    "    df = _get_dcg_at_k(scores)\n",
    "    \n",
    "    dcg_so_far = \"\"\n",
    "    for i, dcg in enumerate(df[\"relevance(i) / log2(i+1)\"]):\n",
    "        if not dcg_so_far:\n",
    "            dcg_so_far = f\"{dcg:2.2f}\"\n",
    "        else:\n",
    "            dcg_so_far = f\"{dcg_so_far} + {dcg:2.2f}\"\n",
    "        print(f\"DCG@{i+1} = {dcg_so_far:<32} = {eval(dcg_so_far):2.2f}\")\n",
    "    \n",
    "    return df𝑟𝑒𝑙𝑖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a304e7",
   "metadata": {},
   "source": [
    "Now we can compare the results from the two models A and B we mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dcg_at_k([5, 2, 4, 0, 1])  # model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dcg_at_k([2, 0, 5, 1, 4])  # model B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a452471",
   "metadata": {},
   "source": [
    "DCG solves the problem with CG but it also has a drawback, it can't be used to compare queries/models that return different number of results.\n",
    "For example consider query A and B where query A returns `[5,2,4]` query B returns `[5,2,4,0,1]`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dcg_at_k([5, 2, 4])  # query A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad94d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dcg_at_k([5, 2, 4, 0, 1])  # query B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f06ff4",
   "metadata": {},
   "source": [
    "Query B got heigher DCG because it returned 5 documents whereas query A only returned 3. We can't say that query B was better than query A.\n",
    "nDCG fixes this issue by adding a normalization factor on top of DCG.\n",
    "\n",
    "$$ nDCG@k(Q) = \\frac{1}{|Q|} * \\sum_{q\\in Q} \\frac{DCG@k(q)}{DCG@k(sorted(rel(q)))} $$\n",
    "\n",
    "|Q| - Number of queries\n",
    "\n",
    "The normalization factor 𝐷𝐶𝐺@𝑘(𝑠𝑜𝑟𝑡𝑒𝑑(𝑟𝑒𝑙(𝑞))) is the Ideal DCG@k. This is calculated by sorting the graded relevance scores returned for a query and then calculating the DCG@k. nDCG@k always lie between 0 and 1.\n",
    "\n",
    "Now we can compare queries/models with different number of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4358d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dcg_at_k(sorted([5, 2, 4], reverse=True))  # Ideal DCG@k for query A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b7791",
   "metadata": {},
   "source": [
    "nDCG@3 for query A = 8.26 / 8.52 = 0.9694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dcg_at_k(sorted([5, 2, 4, 0, 1], reverse=True))  # Ideal DCG@k for query B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e11299",
   "metadata": {},
   "source": [
    "nDCG@5 for query B = 8.65 / 8.95 = 0.9664"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
